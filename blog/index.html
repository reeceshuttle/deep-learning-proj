<html>
  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

    <link rel="shortcut icon" href="images/icon.ico" />
    <style type="text/css">
      body {
      	background-color: #f5f9ff;
      }

      /* Hide both math displays initially, will display based on JS detection */
       .mathjax-mobile, .mathml-non-mobile { display: none; }

       /* Show the MathML content by default on non-mobile devices */
       .show-mathml .mathml-non-mobile { display: block; }
       .show-mathjax .mathjax-mobile { display: block; }

      .content-margin-container {
      	display: flex;
      	width: 100%; /* Ensure the container is full width */
      	justify-content: left; /* Horizontally centers the children in the container */
      	align-items: center;  /* Vertically centers the children in the container */
      }
      .main-content-block {
      	width: 70%; /* Change this percentage as needed */
         max-width: 1100px; /* Optional: Maximum width */
      	background-color: #fff;
      	border-left: 1px solid #DDD;
      	border-right: 1px solid #DDD;
      	padding: 8px 8px 8px 8px;
      	font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
      }
      .margin-left-block {
      		font-size: 14px;
      		width: 15%; /* Change this percentage as needed */
      		max-width: 130px; /* Optional: Maximum width */
      		position: relative;
      		margin-left: 10px;
      		text-align: left;
      		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
      		padding: 5px;
      }
      .margin-right-block {
      		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
      		font-size: 14px;
      		width: 25%; /* Change this percentage as needed */
      		max-width: 256px; /* Optional: Maximum width */
      		position: relative;
      		text-align: left;
      		padding: 10px;  /* Optional: Adds padding inside the caption */
      }

      img {
      		max-width: 100%; /* Make sure it fits inside the container */
      		height: auto;
      		display: block;
      		margin: auto;
      }
      .my-video {
      		max-width: 100%; /* Make sure it fits inside the container */
      		height: auto;
      		display: block;
      		margin: auto;
      }
      /* Hide both video displays initially, will display based on JS detection */
       .vid-mobile, .vid-non-mobile { display: none; }

       /* Show the video content by default on non-mobile devices */
       .show-vid-mobile .vid-mobile { display: block; }
       .show-vid-non-mobile .vid-non-mobile { display: block; }

      a:link,a:visited
      {
      	color: #0e7862; /*#1367a7;*/
      	text-decoration: none;
      }
      a:hover {
      	color: #24b597; /*#208799;*/
      }

      h1 {
      	font-size: 18px;
      	margin-top: 4px;
      	margin-bottom: 10px;
      }

      table.header {
         font-weight: 300;
         font-size: 17px;
         flex-grow: 1;
      	width: 70%;
         max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
      }
      table td, table td * {
          vertical-align: middle;
          position: relative;
      }
      table.paper-code-tab {
          flex-shrink: 0;
          margin-left: 8px;
          margin-top: 8px;
          padding: 0px 0px 0px 8px;
          width: 290px;
          height: 150px;
      }

      .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      	box-shadow:
      	        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
      	        5px 5px 0 0px #fff, /* The second layer */
      	        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
      	        10px 10px 0 0px #fff, /* The third layer */
      	        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
      	margin-top: 5px;
      	margin-left: 10px;
      	margin-right: 30px;
      	margin-bottom: 5px;
      }

      hr {
         height: 1px; /* Sets the height of the line to 1 pixel */
         border: none; /* Removes the default border */
         background-color: #DDD; /* Sets the line color to black */
       }

      div.hypothesis {
      	width: 80%;
      	background-color: #EEE;
      	border: 1px solid black;
      	border-radius: 10px;
      	-moz-border-radius: 10px;
      	-webkit-border-radius: 10px;
      	font-family: Courier;
      	font-size: 18px;
      	text-align: center;
      	margin: auto;
      	padding: 16px 16px 16px 16px;
      }

      div.citation {
         font-size: 0.8em;
         background-color:#fff;
         padding: 10px;
      	height: 200px;
       }

      .fade-in-inline {
      	position: absolute;
      	text-align: center;
      	margin: auto;
      	-webkit-mask-image: linear-gradient(to right,
      																		transparent 0%,
      																		transparent 40%,
      																		black 50%,
      																		black 90%,
      																		transparent 100%);
      	mask-image: linear-gradient(to right,
      															transparent 0%,
      															transparent 40%,
      															black 50%,
      															black 90%,
      															transparent 100%);
      	-webkit-mask-size: 8000% 100%;
      	mask-size: 8000% 100%;
      	animation-name: sweepMask;
      	animation-duration: 4s;
      	animation-iteration-count: infinite;
      	animation-timing-function: linear;
      	animation-delay: -1s;
      }

      .fade-in2-inline {
      		animation-delay: 1s;
      }

      .inline-div {
      		position: relative;
          display: inline-block; /* Makes both the div and paragraph inline-block elements */
          vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
          width: 50px; /* Optional: Adds space between the div and the paragraph */
      }
    </style>

    <title>Deep Learning Project</title>
    <meta property="og:title" content="Deep Learning Project" />
    <meta charset="UTF-8" />
  </head>

  <body>
    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <table class="header" align="left">
          <tr>
            <td colspan="4">
              <span
                style="
                  font-size: 30px;
                  font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */
                "
                >Exploring the Causes & Effects of Quantization-induced
                Degradation in LLMs</span
              >
            </td>
          </tr>
          <tr>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="https://reeceshuttle.me">Reece Shuttleworth</a></span
              >
            </td>
          </tr>

          <tr>
            <td colspan="4" align="left">
              <span style="font-size: 18px">Final project for 6.7960, MIT</span>
            </td>
          </tr>
        </table>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="methods">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>TL;DR.</h1>
        Too Long, Didn't Read.<br /><br />
      </div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block">
        <!-- table of contents here -->
        <div style="position: fixed; max-width: inherit; top: max(20%, 120px)">
          <b style="font-size: 16px">Contents</b><br /><br />
          <a href="#intro">Introduction</a><br /><br />
          <a href="#background">Background</a><br /><br />
          <a href="#methods">Methods</a><br /><br />
          <a href="#analysis">Analysis</a><br /><br />
          <a href="#discussion">Discussion</a><br /><br />
        </div>
      </div>

      <div class="main-content-block">
        <!--You can embed an image like this:-->
        <img src="./images/QiD_loss.png" width="768px" />
      </div>
      <div class="margin-right-block">Caption for the image.</div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Introduction</h1>

        Current LLMs are pre-trained far into the overtrained regime: they are
        trained on many multiples of tokens greater than what is considered to
        be compute optimal (around 20 tokens per model parameter [<a
          href="#ref_8"
          >8</a
        >]). For example, LLaMA-3 8B [<a href="#ref_7">7</a>] was trained on
        more than 15 Trillion tokens, which approaches 2000 tokens per model
        parameter. Even though it is not compute optimal, this extended
        pre-training stage has been shown to improve performance and reduce
        inference costs given a particular level of performance [<a
          href="#ref_6"
          >6</a
        >].

        <br />
        <br />

        Another important method to reduce inference costs is quantization,
        which reduces the precision that weights (and sometimes activations) are
        stored in. This reduction in precision leads to memory and compute
        savings, because you can represent the model in fewer bits and can more
        easily move weights into GPU memory and do computations when the values
        have fewer bits.

        <br />
        <br />

        Generally, after training models are quantized before being served to
        the public. However, recent work has observed an interesting phenomenon
        in which models that are pre-trained for longer have higher loss after
        post-training quantization (PTQ) than models that were trained on fewer
        tokens[<a href="#ref_1">1</a>,<a href="#ref_2">2</a>]. This leads to a
        clear conflict, because pre-training for longer leads to better
        performance and enables smaller models to be used given a fixed level of
        performance.

        <br /><br />

        This observation suggests that there will be a clear limit to the amount
        we can quantize a model, which may place limits on the efficiency gains
        we can get from techniques. While the current findings to not impact
        existing methods because they are occuring at very low bits, like 3 bit,
        as models get pre-trained for longer this observation has the potential
        to also effect current methods.
        <br /><br />

        It is not clear why this phenomenon is occuring and what the
        consequences of it will be. In this work, we aim to investigate the
        causes and effects of this phenomenon. Previous work has shown that LLMs
        with better performance have large activations with a higher frequency
        [<a href="#ref_4">4</a>]. These large activations are difficult to
        quantize without errors and are important for model performance [<a
          href="#ref_4"
          >4</a
        >]. Could these be the culprit behind this degradation in performance?
        Furthermore, this phenomenon has only been observed in loss. What are
        the impacts of this on downstream task performance? Lastly, can we find
        the specific part of the model that is breaking and causing this
        phenomenon?

        <br /><br />

        Our results can be reproduced with our codebase, which can be found
        <a
          href="https://github.com/reeceshuttle/deep-learning-proj"
          target="_blank"
          ><u>here.</u></a
        >
        <br />
      </div>
      <div class="margin-right-block">
        Margin note that clarifies some detail #main-content-block for intro
        section.
      </div>
    </div>

    <div class="content-margin-container" id="background">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Background / Related Work</h1>
        Put Background here.<br /><br />
        <h4>Quantization</h4>
        In this work, we investigate two kinds of quantization
        <h4>Quantization-induced Degradation</h4>

        <h4>Large Activations in LLMs</h4>
        <img src="./images/QiD_task_perf.png" width="1024px" />
      </div>
      <div class="margin-right-block">Caption for the image.</div>
    </div>

    <div class="content-margin-container" id="methods">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Methods / Experiments</h1>
        Put methods here.<br /><br />
        <img src="./images/16bit_statistics.png" width="1024px" />
      </div>
      <div class="margin-right-block">Caption for the image.</div>
    </div>

    <div class="content-margin-container" id="analysis">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Results & Analysis</h1>
        Put results here.<br /><br />

        <img src="./images/error_norm.png" width="1024px" />
      </div>
      <div class="margin-right-block">Caption for the image.</div>
    </div>

    <!-- <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Another section</h1>
        In this section we embed a video:
        <video class="my-video" loop autoplay muted style="width: 725px">
          <source src="./images/mtsh.mp4" type="video/mp4" />
        </video>
      </div>
      <div class="margin-right-block">
        A caption for the video could go here.
      </div>
    </div> -->

    <div class="content-margin-container" id="discussion">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Discussion</h1>
        Let's end with some discussion of the implications and limitations. is
        something like superposition to blame? could quantizing the att_proj in
        higher bits and the rest in lower bits fix this problem?
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="citations">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <div class="citation" id="references" style="height: auto">
          <br />
          <span style="font-size: 16px">References:</span><br /><br />
          <a id="ref_1"></a>[1]
          <a href="https://arxiv.org/pdf/2411.04330"
            >Scaling Laws For Precision</a
          >, Kumar et al. 2024<br /><br />

          <a id="ref_2"></a>[2]
          <a href="https://arxiv.org/pdf/2411.17691"
            >Low-Bit Quantization Favors Undertrained LLMs</a
          >, Ouyang et al. 2024<br /><br />

          <a id="ref_3"></a>[3]
          <a
            href="https://www.databricks.com/blog/calibrating-mosaic-evaluation-gauntlet"
            >Calibrating the Mosaic Evaluation Gauntlet</a
          >, Tessa Barton, 2024<br /><br />

          <a id="ref_4"></a>[4]
          <a href="https://arxiv.org/pdf/2208.07339">LLM.int8()</a>, Dettmers et
          al. 2022<br /><br />

          <a id="ref_5"></a>[5]
          <a href="https://arxiv.org/pdf/2306.00978">AWQ</a>, Lin et al. 2023<br /><br />

          <a id="ref_6"></a>[6]
          <a href="https://arxiv.org/pdf/2302.13971">LLaMA</a>, Touvron et al.
          2023<br /><br />

          <a id="ref_7"></a>[7]
          <a href="https://arxiv.org/pdf/2407.21783">LLaMA3</a>, Grattafiori et
          al. 2024<br /><br />

          <a id="ref_8"></a>[8]
          <a href="https://arxiv.org/pdf/2203.15556">Chinchilla</a>, Hoffmann et
          al. 2022<br /><br />
        </div>
      </div>
      <div class="margin-right-block">
        <!-- margin notes for reference block here -->
      </div>
    </div>
  </body>
</html>
